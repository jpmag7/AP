{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Downloading tensorflow_intel-2.12.0-cp310-cp310-win_amd64.whl (272.8 MB)\n",
      "     -------------------------------------- 272.8/272.8 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.2.0)\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "     --------------------------------------- 14.6/14.6 MB 23.4 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 19.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.22.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "     ------------------------------------- 420.6/420.6 kB 25.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.4.0)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 25.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (23.3.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (16.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jpmag\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (23.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (65.6.3)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "     ------------------------------------- 440.7/440.7 kB 26.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.8.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 22.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.53.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\jpmag\\appdata\\roaming\\python\\python310\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.10.1)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.0.4-cp310-cp310-win_amd64.whl (98 kB)\n",
      "     ---------------------------------------- 98.1/98.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jpmag\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.4.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.17.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.28.2)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jpmag\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439795 sha256=330f260838ac27affdf190de7ca326742c3667b2accbcbb3787b4d08b2c566a0\n",
      "  Stored in directory: c:\\users\\jpmag\\appdata\\local\\pip\\cache\\wheels\\09\\6f\\35\\a8fac8b61de8e0d9eb07988481528898561923e260b1fa7d2f\n",
      "Successfully built jax\n",
      "Installing collected packages: wrapt, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, keras, ml-dtypes, jax, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acesso negado: 'C:\\\\Users\\\\jpmag\\\\.conda\\\\envs\\\\ap\\\\Lib\\\\site-packages\\\\~rapt\\\\_wrappers.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-nlp --upgrade\n",
    "#!pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True)\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras_nlp\n",
    "from keras import backend\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = open(\"Shakespeare.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = datafile.read()\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "embed_dim = math.ceil(math.log2(len(vocab)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 48, 57, 58, 59, 2, 16, 48, 59, 48, 65, 44, 53, 11, 1, 15, 44, 45, 54, 57]\n"
     ]
    }
   ],
   "source": [
    "mask = 0\n",
    "stoi = {ch:i+1 for i,ch in enumerate(vocab)}\n",
    "itos = {i+1:ch for i,ch in enumerate(vocab)}\n",
    "tokenize = lambda s: [stoi[c] for c in s]\n",
    "detokenize = lambda l: \"\".join([itos[i] for i in l])\n",
    "tokenized_text = tokenize(text)\n",
    "text_size = len(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "print(tokenized_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_sample(input_size):\n",
    "    end = random.randint(0, text_size)\n",
    "    start = max(end - input_size, 0)\n",
    "    sample = tokenized_text[start : end]\n",
    "    \n",
    "    if (input_size - len(sample)) > 0:\n",
    "        sample = [mask] * (input_size - len(sample)) + sample\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def get_train_epoch(epoch_size, input_size):\n",
    "    X = []\n",
    "    for _ in range(epoch_size):\n",
    "        x = get_train_sample(input_size)\n",
    "        X.append(x)\n",
    "    X = tf.reshape(tf.constant(X), (len(X), input_size))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       "array([[26, 64,  2, 59, 54],\n",
       "       [52, 44,  2, 45, 54]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_epoch(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 128, 64)      4224        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " position_embedding_3 (Position  (None, 128, 64)     8192        ['embedding_3[0][0]']            \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 128, 64)     0           ['embedding_3[0][0]',            \n",
      " ambda)                                                           'position_embedding_3[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 128, 64)     16640       ['tf.__operators__.add_23[0][0]',\n",
      " HeadAttention)                                                   'tf.__operators__.add_23[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 128, 64)     0           ['tf.__operators__.add_23[0][0]',\n",
      " ambda)                                                           'multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 128, 64)     128         ['tf.__operators__.add_24[0][0]']\n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 128, 256)     16640       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 128, 64)      16448       ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 128, 64)     0           ['layer_normalization[0][0]',    \n",
      " ambda)                                                           'dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_25[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 128, 64)     16640       ['layer_normalization_1[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 128, 64)     0           ['layer_normalization_1[0][0]',  \n",
      " ambda)                                                           'multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_26[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 128, 256)     16640       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 128, 64)      16448       ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 128, 64)     0           ['layer_normalization_2[0][0]',  \n",
      " ambda)                                                           'dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_27[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 128, 64)     16640       ['layer_normalization_3[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 128, 64)     0           ['layer_normalization_3[0][0]',  \n",
      " ambda)                                                           'multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_28[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 128, 256)     16640       ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 128, 64)      16448       ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 128, 64)     0           ['layer_normalization_4[0][0]',  \n",
      " ambda)                                                           'dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_29[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 128, 64)     16640       ['layer_normalization_5[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 128, 64)     0           ['layer_normalization_5[0][0]',  \n",
      " ambda)                                                           'multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_30[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 128, 256)     16640       ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 128, 64)      16448       ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 128, 64)     0           ['layer_normalization_6[0][0]',  \n",
      " ambda)                                                           'dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_31[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 128, 64)     16640       ['layer_normalization_7[0][0]',  \n",
      " HeadAttention)                                                   'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 128, 64)     0           ['layer_normalization_7[0][0]',  \n",
      " ambda)                                                           'multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_32[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 128, 256)     16640       ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 128, 64)      16448       ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 128, 64)     0           ['layer_normalization_8[0][0]',  \n",
      " ambda)                                                           'dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 128, 64)     128         ['tf.__operators__.add_33[0][0]']\n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 128, 64)      4160        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 128, 66)      4290        ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 270,786\n",
      "Trainable params: 270,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_size=128, num_blocks=5, embed_dim=64):\n",
    "    inputs = Input(shape=(input_size,))\n",
    "    embs = Embedding(vocab_size + 1, embed_dim)(inputs)\n",
    "    x = embs + keras_nlp.layers.PositionEmbedding(input_size)(embs)\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        x = x + MultiHeadAttention(\n",
    "            int(math.sqrt(embed_dim)),\n",
    "            int(math.sqrt(embed_dim))\n",
    "        )(x, x, use_causal_mask=True)\n",
    "        x = LayerNormalization()(x)\n",
    "        x1 = Dense(embed_dim * 4, activation=\"leaky_relu\")(x)\n",
    "        x1 = Dense(embed_dim, activation=\"leaky_relu\")(x1)\n",
    "        x = LayerNormalization()(x + x1)\n",
    "\n",
    "    x = Dense(embed_dim, activation=\"leaky_relu\")(x)\n",
    "    x = Dense(vocab_size + 1, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_size = 128\n",
    "model = build_model(input_size=input_size, num_blocks=5, embed_dim=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 547s 4s/step - loss: 2.8172 - accuracy: 0.2361\n",
      " 70/128 [===============>..............] - ETA: 4:00 - loss: 2.4628 - accuracy: 0.2821"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      2\u001b[0m     X \u001b[39m=\u001b[39m get_train_epoch(\u001b[39m4096\u001b[39m\u001b[39m*\u001b[39m\u001b[39m8\u001b[39m, input_size \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], X[:,\u001b[39m1\u001b[39;49m:], shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    X = get_train_epoch(4096*8, input_size + 1)\n",
    "    model.fit(X[:,:-1], X[:,1:], shuffle=True, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise theire thim,\n",
      "Tho he t andous that sengr s tene atoul tharereal myoulle me thin h has t s m thango aras tero st herile s\n",
      "Thereas se tongreso ses theraneal tof h m thoras t thenoure as t t arerengrillo s thinge hangheal m ton hen s a"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m tokens \u001b[39m=\u001b[39m [mask] \u001b[39m*\u001b[39m (input_size \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(tokens)) \u001b[39m+\u001b[39m tokens\n\u001b[0;32m     16\u001b[0m tokens \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mconstant(tokens), (\u001b[39m1\u001b[39m, input_size))\n\u001b[1;32m---> 17\u001b[0m output \u001b[39m=\u001b[39m model(tokens)[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m top_k \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mtop_k(output, \u001b[39m5\u001b[39m)\n\u001b[0;32m     20\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(top_k\u001b[39m.\u001b[39mindices, p\u001b[39m=\u001b[39mtop_k\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mnumpy()\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msum(top_k\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mnumpy()))\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py:311\u001b[0m, in \u001b[0;36mLayerNormalization.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m tensor_shape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(inputs)\n\u001b[0;32m    310\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, ndims):\n\u001b[1;32m--> 311\u001b[0m     dim_tensor \u001b[39m=\u001b[39m tensor_shape[dim]\n\u001b[0;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m dim \u001b[39m<\u001b[39m axis[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    313\u001b[0m         pre_dim \u001b[39m=\u001b[39m pre_dim \u001b[39m*\u001b[39m dim_tensor\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1096\u001b[0m, in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1094\u001b[0m   var_empty \u001b[39m=\u001b[39m constant([], dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\n\u001b[0;32m   1095\u001b[0m   packed_begin \u001b[39m=\u001b[39m packed_end \u001b[39m=\u001b[39m packed_strides \u001b[39m=\u001b[39m var_empty\n\u001b[1;32m-> 1096\u001b[0m \u001b[39mreturn\u001b[39;00m strided_slice(\n\u001b[0;32m   1097\u001b[0m     tensor,\n\u001b[0;32m   1098\u001b[0m     packed_begin,\n\u001b[0;32m   1099\u001b[0m     packed_end,\n\u001b[0;32m   1100\u001b[0m     packed_strides,\n\u001b[0;32m   1101\u001b[0m     begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[0;32m   1102\u001b[0m     end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[0;32m   1103\u001b[0m     shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask,\n\u001b[0;32m   1104\u001b[0m     new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[0;32m   1105\u001b[0m     ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[0;32m   1106\u001b[0m     var\u001b[39m=\u001b[39;49mvar,\n\u001b[0;32m   1107\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1269\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[39mif\u001b[39;00m strides \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1267\u001b[0m   strides \u001b[39m=\u001b[39m ones_like(begin)\n\u001b[1;32m-> 1269\u001b[0m op \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mstrided_slice(\n\u001b[0;32m   1270\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49minput_,\n\u001b[0;32m   1271\u001b[0m     begin\u001b[39m=\u001b[39;49mbegin,\n\u001b[0;32m   1272\u001b[0m     end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m   1273\u001b[0m     strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[0;32m   1274\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1275\u001b[0m     begin_mask\u001b[39m=\u001b[39;49mbegin_mask,\n\u001b[0;32m   1276\u001b[0m     end_mask\u001b[39m=\u001b[39;49mend_mask,\n\u001b[0;32m   1277\u001b[0m     ellipsis_mask\u001b[39m=\u001b[39;49mellipsis_mask,\n\u001b[0;32m   1278\u001b[0m     new_axis_mask\u001b[39m=\u001b[39;49mnew_axis_mask,\n\u001b[0;32m   1279\u001b[0m     shrink_axis_mask\u001b[39m=\u001b[39;49mshrink_axis_mask)\n\u001b[0;32m   1281\u001b[0m parent_name \u001b[39m=\u001b[39m name\n\u001b[0;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m var \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10671\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10669\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m  10670\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 10671\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m  10672\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mStridedSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, begin, end, strides, \u001b[39m\"\u001b[39;49m\u001b[39mbegin_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m  10673\u001b[0m       begin_mask, \u001b[39m\"\u001b[39;49m\u001b[39mend_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, end_mask, \u001b[39m\"\u001b[39;49m\u001b[39mellipsis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, ellipsis_mask,\n\u001b[0;32m  10674\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mnew_axis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, new_axis_mask, \u001b[39m\"\u001b[39;49m\u001b[39mshrink_axis_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, shrink_axis_mask)\n\u001b[0;32m  10675\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m  10676\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"\"\"First Citizen:\n",
    "We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely;\n",
    "but they think we are too dear: the leanness that\n",
    "afflicts us, the object of our misery, is as an\n",
    "inventory to particularise their\"\"\"\n",
    "\n",
    "print(text, end=\"\")\n",
    "for _ in range(256):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = tokens[max(0, len(tokens) - input_size):]\n",
    "    tokens = [mask] * (input_size - len(tokens)) + tokens\n",
    "    \n",
    "    tokens = tf.reshape(tf.constant(tokens), (1, input_size))\n",
    "    output = model(tokens)[0][-1]\n",
    "    \n",
    "    top_k = tf.math.top_k(output, 5)\n",
    "    output = np.random.choice(top_k.indices, p=top_k.values.numpy()/np.sum(top_k.values.numpy()))\n",
    "    if output == 0 or output > vocab_size:\n",
    "        print(\"\\nSTOPED\\n\")\n",
    "        break\n",
    "    output = detokenize([output])\n",
    "    print(output, end=\"\")\n",
    "    text = text + \"\" + output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
