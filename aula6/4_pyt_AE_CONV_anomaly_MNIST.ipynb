{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autoencoder simples CONV para deteção de anomalias\n",
    "Neste caso, para deteção de imagens que não pertencem o MNIST dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normal Data*\n",
    "\n",
    "\n",
    "![image_info](https://miro.medium.com/max/1400/1*P7aFcjaMGLwzTvjW3sD-5Q.jpeg)\n",
    "\n",
    "\n",
    "*Anomaly*\n",
    "\n",
    "\n",
    "![image_info](https://miro.medium.com/max/1400/1*-bHmKt4JPKIAmmyO47OYnQ.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch conv for binary classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "    \n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#path para guardar o dataset\n",
    "PATH = './'\n",
    "PATH_TRAIN = './mnist_train.csv'\n",
    "PATH_TEST = './mnist_test.csv'\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device management \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Preparar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#buscar o dataset utilizando os CSVs e uma classe para o dataset\n",
    "\n",
    "# definição classe para o dataset\n",
    "class CSVDataset(Dataset):\n",
    "    # ler o dataset\n",
    "    def __init__(self, path_train, path_test):\n",
    "        # ler o ficheiro csv para um dataframe\n",
    "        df_train = pd.read_csv(path_train, header=0)\n",
    "        df_test = pd.read_csv(path_test, header=0)\n",
    "        # separar os inputs e os outputs\n",
    "        self.x_train = df_train.values[:, 1:]\n",
    "        self.x_train = self.x_train.reshape(len(self.x_train), 1, 28, 28)\n",
    "        xmax, xmin = self.x_train.max(), self.x_train.min()\n",
    "        self.x_train  = (self.x_train - xmin)/(xmax - xmin)\n",
    "        self.y_train = df_train.values[:, 0]\n",
    "        self.x_test = df_test.values[:, 1:]\n",
    "        self.x_test = self.x_test.reshape(len(self.x_test), 1, 28, 28)\n",
    "        xmax, xmin = self.x_test.max(), self.x_test.min()\n",
    "        self.x_test  = (self.x_test - xmin)/(xmax - xmin)\n",
    "        self.y_test = df_test.values[:, 0]\n",
    "        # garantir que os inputs e labels sejam floats\n",
    "        self.x_train = self.x_train.astype('float32')\n",
    "        self.x_test = self.x_test.astype('float32')\n",
    "        self.y_train = self.y_train.astype('long')\n",
    "        self.y_test = self.y_test.astype('long')\n",
    "        \n",
    "    # numero de casos de treino no dataset\n",
    "    def __len_train__(self):\n",
    "        return len(self.x_train)\n",
    "     # numero de casos de teste no dataset\n",
    "    def __len_test__(self):\n",
    "        return len(self.x_test)\n",
    "    \n",
    "    # retornar um caso\n",
    "    def __getitem_train__(self, idx):\n",
    "        return [self.x_train[idx], self.y_train[idx]]\n",
    "     # retornar um caso\n",
    "    def __getitem_test__(self, idx):\n",
    "        return [self.x_test[idx], self.y_test[idx]]\n",
    "    \n",
    "    # retornar indeces para casos de treino de de teste em formato flat (vetor)\n",
    "    def get_splits(self):\n",
    "        x_train  = torch.from_numpy(np.array(self.x_train))\n",
    "        y_train  = torch.from_numpy(np.array(self.y_train))\n",
    "        x_test  = torch.from_numpy(np.array(self.x_test))\n",
    "        y_test  = torch.from_numpy(np.array(self.y_test))\n",
    "        train = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "        test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "        return train, test \n",
    "    \n",
    "# preparar o dataset\n",
    "def prepare_data_flat(path_train, path_test):\n",
    "    # criar uma instancia do dataset\n",
    "    dataset = CSVDataset(path_train, path_test)\n",
    "    # calcular split\n",
    "    train, test = dataset.get_splits()\n",
    "    # preparar data loaders\n",
    "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    train_dl_all = DataLoader(train, batch_size=len(train), shuffle=False)\n",
    "    test_dl_all = DataLoader(test, batch_size=len(test), shuffle=False)\n",
    "    return train_dl, test_dl, train_dl_all, test_dl_all\n",
    "\n",
    "# preparar os dados\n",
    "train_dl, test_dl,  train_dl_all, test_dl_all = prepare_data_flat(PATH_TRAIN, PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Visualizar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualização das imagens\n",
    "def visualize_mnist_images_flat(dl):\n",
    "    # get one batch of images\n",
    "    i, (inputs, targets) = next(enumerate(dl))\n",
    "    print(inputs.shape)\n",
    "    print(inputs.shape)\n",
    "    print(inputs.shape)\n",
    "    # plot some images\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.grid(b=None)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(inputs[i][0], cmap='gray')\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Ler o modelo previamente treinado em \"2_pytorch_AE_CONV_treino_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AE_CONV                                  [128, 1, 28, 28]          --\n",
      "├─Sequential: 1-1                        [128, 1, 3, 3]            --\n",
      "│    └─Conv2d: 2-1                       [128, 128, 22, 22]        6,400\n",
      "│    └─BatchNorm2d: 2-2                  [128, 128, 22, 22]        256\n",
      "│    └─ReLU: 2-3                         [128, 128, 22, 22]        --\n",
      "│    └─MaxPool2d: 2-4                    [128, 128, 11, 11]        --\n",
      "│    └─Conv2d: 2-5                       [128, 64, 9, 9]           73,792\n",
      "│    └─BatchNorm2d: 2-6                  [128, 64, 9, 9]           128\n",
      "│    └─ReLU: 2-7                         [128, 64, 9, 9]           --\n",
      "│    └─MaxPool2d: 2-8                    [128, 64, 4, 4]           --\n",
      "│    └─Conv2d: 2-9                       [128, 1, 3, 3]            257\n",
      "│    └─BatchNorm2d: 2-10                 [128, 1, 3, 3]            2\n",
      "│    └─ReLU: 2-11                        [128, 1, 3, 3]            --\n",
      "├─Sequential: 1-2                        [128, 1, 28, 28]          --\n",
      "│    └─ConvTranspose2d: 2-12             [128, 64, 9, 9]           1,664\n",
      "│    └─BatchNorm2d: 2-13                 [128, 64, 9, 9]           128\n",
      "│    └─ReLU: 2-14                        [128, 64, 9, 9]           --\n",
      "│    └─ConvTranspose2d: 2-15             [128, 128, 23, 23]        401,536\n",
      "│    └─BatchNorm2d: 2-16                 [128, 128, 23, 23]        256\n",
      "│    └─ReLU: 2-17                        [128, 128, 23, 23]        --\n",
      "│    └─ConvTranspose2d: 2-18             [128, 1, 28, 28]          4,609\n",
      "│    └─BatchNorm2d: 2-19                 [128, 1, 28, 28]          2\n",
      "│    └─Sigmoid: 2-20                     [128, 1, 28, 28]          --\n",
      "==========================================================================================\n",
      "Total params: 489,030\n",
      "Trainable params: 489,030\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 28.83\n",
      "==========================================================================================\n",
      "Input size (MB): 0.40\n",
      "Forward/backward pass size (MB): 288.41\n",
      "Params size (MB): 1.96\n",
      "Estimated Total Size (MB): 290.77\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torchinfo\\torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "c:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE_CONV(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (9): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(1, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(64, 128, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 1, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models_mnist #modulo python com os modelos       \n",
    "    \n",
    "# definir a rede neuronal\n",
    "model = models_mnist.AE_CONV()\n",
    "\n",
    "# ler o modelo\n",
    "SAVED_MODEL =  'AE_CONV_MNIST.pth'\n",
    "#model= torch.load(SAVED_MODEL)\n",
    "model= torch.load(SAVED_MODEL, map_location ='cpu')\n",
    "model.eval()\n",
    "#visualizar a rede\n",
    "print(summary(model, input_size=(BATCH_SIZE,  1,28,28), verbose=0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usar o Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28, 28)\n",
      "img.shape: torch.Size([1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.float32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m _, (inputs, targets) \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39menumerate\u001b[39m(test_dl))\n\u001b[0;32m     54\u001b[0m \u001b[39m# se a imagem imagem_nao_digito.png não for um digito do genero em que foi treinado então a distancia entre os dois vetores será muito grande.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m anomaly_detection(model, img, inputs, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36manomaly_detection\u001b[1;34m(model, img_anomaly, img_list, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m img_list \u001b[39m=\u001b[39m img_list\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m img_anomaly\u001b[39m=\u001b[39m img_anomaly\u001b[39m.\u001b[39mto(device)   \n\u001b[1;32m---> 37\u001b[0m pred_img_anomaly,_ \u001b[39m=\u001b[39m model(img_anomaly)\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimg_anomaly.shape: \u001b[39m\u001b[39m{\u001b[39;00mimg_anomaly\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpred_img_anomaly.shape: \u001b[39m\u001b[39m{\u001b[39;00mpred_img_anomaly\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\OneDrive\\Ambiente de Trabalho\\AP\\aula6\\models_mnist.py:235\u001b[0m, in \u001b[0;36mAE_CONV.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m--> 235\u001b[0m     ls\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    236\u001b[0m     z\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(ls)\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m z,ls\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jpmag\\.conda\\envs\\ap\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n",
      "\u001b[1;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "#Podemos utilizar este modelo para deteção de anomalias (imagens que não são digitos)\n",
    "\n",
    "# Processar a imagem\n",
    "def process_image(image_path,w,h):\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    # Resize para alteração da dimensão mas a manter o aspect ratio\n",
    "    img = img.resize((w, int(h*(height/width))) if width < height else (int(w*(width/height)), h))\n",
    "    # nbter as dimensões novas\n",
    "    width, height = img.size\n",
    "    # Definir as coordenadas para o centro de w x h\n",
    "    left = (width - w)/2\n",
    "    top = (height - h)/2\n",
    "    right = (width + w)/2\n",
    "    bottom = (height + h)/2\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    img = ImageOps.grayscale(img)\n",
    "    # Converter para array numpy\n",
    "    img = np.array(img)\n",
    "    print(f'shape:{img.shape}')\n",
    "    # Normalizar\n",
    "    xmax, xmin = img.max(), img.min()\n",
    "    img  = (img - xmin)/(xmax - xmin)\n",
    "    # Adicionar uma quarta dimensão ao início para indicar o batch size\n",
    "    img = img[np.newaxis,:]\n",
    "    # Converter num tensor torch\n",
    "    image = torch.from_numpy(img)\n",
    "    image = image.float()\n",
    "    #image=image.view(1,w*h) #fazer o flat do 28x28 para ficar como o mnist\n",
    "    return image\n",
    "\n",
    "def anomaly_detection(model, img_anomaly, img_list, idx): #img shape (784,1)\n",
    "    print(img_list.shape)\n",
    "    print(img_list.dtype) \n",
    "    img_list = img_list.to(device)\n",
    "    img_anomaly= img_anomaly.to(device)   \n",
    "    pred_img_anomaly,_ = model(img_anomaly)\n",
    "    print(f'img_anomaly.shape: {img_anomaly.shape}')\n",
    "    print(f'pred_img_anomaly.shape: {pred_img_anomaly.shape}')\n",
    "    dist_pred_img = np.linalg.norm(img_anomaly[0].cpu().detach().numpy() - pred_img_anomaly[0].cpu().detach().numpy())  #Distancia de não digito: 22.185663\n",
    "    print(\"Distancia de não digito:\",dist_pred_img)    \n",
    "    pred_img_list,_ = model(img_list)\n",
    "    print(f'pred_img_list.shape: {pred_img_list[idx].shape}')\n",
    "    dist_img1 = np.linalg.norm(img_list[idx].cpu().detach().numpy() - pred_img_list[idx].cpu().detach().numpy())  #Distancia de não digito: 22.185663\n",
    "    print(\"Distancia de  digito1:\",dist_img1)  \n",
    "\n",
    "\n",
    "ANOMALIA = 'imagem_nao_digito.png'\n",
    "#ANOMALIA = 'mnist_reconstruction_in.png'\n",
    "\n",
    "img = process_image(ANOMALIA,28,28)\n",
    "print(f'img.shape: {img.shape}')\n",
    "_, (inputs, targets) = next(enumerate(test_dl))\n",
    "# se a imagem imagem_nao_digito.png não for um digito do genero em que foi treinado então a distancia entre os dois vetores será muito grande.\n",
    "anomaly_detection(model, img, inputs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
